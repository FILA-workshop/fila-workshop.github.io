<html lang="en"> 
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>FILA 2020</title>

    <style type="text/css">
    .skip {
        position: absolute;
        top: -1000px;
        left: -1000px;
        height: 1px;
        width: 1px;
        text-align: left;
        overflow: hidden;
    }
    
    a.skip:active, 
    a.skip:focus, 
    a.skip:hover {
        left: 0; 
        top: 0;
        width: auto; 
        height: auto; 
        overflow: visible; 
    }
    </style>
  
    <style>
    table, th, td {
        border: 1px solid black;
        border-collapse: separate;
        border-spacing: 5px;
    }
    th, td {
        padding: 5px;
    }
    </style>
    
    <style type="text/css">
    <!--
    .tab { margin-left: 40px; }
    -->
    </style>

    <!-- Bootstrap core CSS -->
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">


    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">
    <link href="custom.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
<a href="#content" class="skip">Skip to content</a>
<div id="content">

    <nav class="navbar navbar-inverse navbar-custom navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">FILA 2020</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <!--li class="active"><a href="#">Home</a></li-->
            <!-- <li><a href="#program">Program</a></li> -->
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#call">Call for Papers</a></li>
            <!-- <li><a href="#awards">Awards</a></li> -->
            <li><a href="#dates">Important Dates</a></li>
            <li><a href="#guidelines">Paper Guidelines</a></li>
            <!--li><a href="#special">Special Issue</a></li-->
            <li><a href="#keynote">Keynote Speakers</a></li>
            <li><a href="#organization">Organization</a></li>
            <!--li><a href="#past">Past Workshops</a></li-->
          </ul>

            <!-- <li><a href="https://twitter.com/search?q=%23parlearning"><span class="glyphicon glyphicon-user"></span> Sign Up</a></li> -->
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
<div class="text-center">
  <br>
  <br>
  <br>
      <h2>International Workshop on Fair and Interpretable Learning Algorithms (FILA 2020)</h2>
        <p>December 10 - 13, 2020<br>
        Atlanta, GA, USA</p>
  <p><b>In conjunction with the <a href="http://bigdataieee.org/BigData2020/" target="_blank">IEEE International Conference on Big Data (IEEE BigData 2020)</a></b><br>
December 10 - 13, 2020<br>
Atlanta Marriott Marquis<br>
Atlanta, GA, USA<br>
<!--a href="https://www.kdd.org/kdd2020/"><img src="./kdd2020_Logo_black.gif" alt="KDD 2020 logo" width="10%"></a-->
  </p>
</div>
</div>


<!-- 
<div>
  <h4>Best Paper Award</h4>
A Best Paper Award with a prize of $300 and certificate will be presented at the workshop. The award is sponsored by Huawei Technologies Co. Ltd.
<br>
<img src="http://www.huawei.com/ucmf/groups/public/documents/webasset/hw_000353.jpg">
</div>


<div>
  <h4>Best Paper Award</h4>
The Best Paper Award goes to Azalia Mirhoseini, Bita Rouhani, Ebrahim Songhori, and Farinaz Koushanfar for their paper <i>ExtDict: Extensible Dictionaries for Data- and Platform-Aware Large-Scale Learning</i>. Congratulations!
<p>
</div>
-->

<!-- 
      <div id="program">
        <h4>Advance Program</h4>
      

<table  class="table" style="width:100%">
  <tr>
    <th>Time</th>
    <th>Title</th>
    <th>Authors/Speaker</th>
  </tr>
  <tr>
    <td>8:15-8:30am</td>
    <td colspan=2><i>Opening remarks</i></td>
  </tr>
  <tr>
    <td>8:30-9:30am</td>
    <td><a href="#keynote1">Invited Talk 1: Scaling Deep Learning Algorithms on Extreme Scale Architectures</a></td>
    <td><a href="#keynote1">Abhinav Vishnu</a>, Principal member of technical staff, AMD, USA</td>
  </tr>
  <tr>
    <td>9:30-10:00am</td>
    <td colspan=2><i>Break</i></td>
  </tr>
  <tr>
    <td>10:00-10:30am</td>
    <td>Near-Optimal Straggler Mitigation for Distributed Gradient Methods (ParLearning-01)</td>
    <td>Songze Li, Seyed Mohammadreza Mousavi Kalan, A. Salman Avestimehr and Mahdi Soltanolkotabi</td>
  </tr>
  <tr>
    <td>10:30-11:00am</td>
    <td>Streaming Tiles: Flexible Implementation of Convolution Neural Networks Inference on Manycore Architectures (ParLearning-02)</td>
    <td>Nesma Rezk, Madhura Purnaprajna and Zain Ul-Abdin</td>
  </tr>
  <tr>
    <td>11:00-12:0pm</td>
    <td><a href="#keynote2">Invited Talk 2: Model Parallelism optimization with deep reinforcement learning</a></td>
    <td><a href="#keynote2">Azalia Mirhoseini</a>, Google Brain, USA</td>
  </tr>
  <tr>
    <td>12:00-1:30pm</td>
    <td colspan=2><i>Lunch</i></td>
  </tr>
    <tr>
    <td>1:30-2:30pm</td>
    <td><a href="#keynote3">Invited Talk 3: Introduction to Snap Machine Learning</a></td>
    <td><a href="#keynote3">Thomas Parnell</a>, IBM Research – Zurich, Switzerland</td>
  </tr>
  <tr>
    <td>2:30-3:00pm</td>
    <td>Parallel Huge Matrix Multiplication on a Cluster with GPGPU Accelerators (ParLearning-03)</td>
    <td>Seungyo Ryu and Dongseung Kim</td>
  </tr>
  <tr>
    <td>3:00-3:30pm</td>
    <td colspan=2><i>Break</i></td>
  </tr>
    <tr>
    <td>3:30-4:00pm</td>
    <td><a href="#keynote4">Invited Talk 4: Matrix Factorization on GPUs: A Tale of Two Algorithms</a></td>
    <td><a href="#keynote4">Wei Tan</a>, Citadel LLC, USA</td>
  </tr>
  <tr>
    <td>4:00-4:30pm</td>
    <td>A Study of Clustering Techniques and Hierarchical Matrix Formats for Kernel Ridge Regression (ParLearning-04)</td>
    <td>Elizaveta Rebrova, Gustavo Chávez, Yang Liu, Pieter Ghysels and Xiaoye Sherry Li</td>
  </tr>
  <tr>
    <td>4:30-5:00pm</td>
    <td><i>Panel Discussion</i></td>
    <td>Azalia Mirhoseini, Thomas Parnell, Wei Tan</td>
  </tr>
  </table>    

-->

<!-- 
      <div id="keynote1">
        <h4>Invited talk 1</h4>
      </div>
<p>
<b>Abhinav Vishnu</b>, Principal member of technical staff, AMD, USA
</p>
<p><b>Scaling Deep Learning Algorithms on Extreme Scale Architectures</b></p>
<p><b>Abstract:</b> 
Deep Learning (DL) is ubiquitous. Yet leveraging distributed memory systems for DL algorithms is incredibly hard. In this talk, we will present approaches to bridge this critical gap.  We will start by scaling DL algorithms on large scale systems such as leadership class facilities (LCFs). Specifically, we will: 1) present our TensorFlow and Keras runtime extensions which require negligible changes in user-code for scaling DL implementations, 2) present communication-reducing/avoiding techniques for scaling DL implementations, 3) present  approaches on fault tolerant DL implementations, and 4) present research on semi-automatic pruning of DNN topologies. We will provide pointers and discussion on the general availability of our research under the umbrella of Machine Learning Toolkit on Extreme Scale (MaTEx) available at <a href="http://github.com/matex-org/matex">http://github.com/matex-org/matex</a></p>
<p><b>Bio:</b>  
Abhinav Vishnu is a Principal Member of Technical Staff at AMD Research. He focuses on designing extreme scale Deep Learning algorithms that are capable of execution on supercomputers and cloud computing systems.  The specific objectives are to design user-transparent distributed TensorFlow;  novel communication reducing/approximation techniques for DL algorithms; fault tolerant Deep Learning/Machine Learning algorithms; multi-dimensional deep neural networks and applications of these techniques on several domains.  His research is publicly available as Machine Learning Toolkit for Extreme Scale (MaTEx) at <a href="http://github.com/matex-org/matex">http://github.com/matex-org/matex</a></p>

      <div id="keynote2">
        <h4>Invited talk 2</h4>
      </div>
<p>
<b>Azalia Mirhoseini</b>, Google Brain, USA
</p>
<p><b>Model Parallelism optimization with deep reinforcement learning</b></p>
<p><b>Abstract:</b>  
The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this talk, I will present some of our recent efforts on learning to optimize model parallelism for TensorFlow computational graphs. Key to our method is the use of deep reinforcement learning to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the deep model. Our main result is that on important computer vision, language modeling and neural machine translation tasks, our model finds non-trivial ways to parallelise the model that outperform hand-crafted heuristics and traditional algorithmic methods.</p>
<p><b>Bio:</b>  
Azalia Mirhoseini is a Research Scientist at Google Brain where she focuses on machine learning approaches to solve problems in computer systems and metalearning. Before Google, she was a Ph.D. student in Electrical and Computer Engineering at Rice University. Her work has been published at several conference and journal venues, including ICML, ICLR, DAC, ICCAD, SIGMETRICS, IEEE TNNLS, and ACM TRETS. She has received a number of awards, including the Best Ph.D. Thesis Award at Rice, fellowships from IBM Research, Microsoft Research, Schlumberger, and a Gold Medal in the National Math Olympiad in Iran. </p>


      <div id="keynote3">
        <h4>Invited talk 3</h4>
      </div>
<p>
<b>Thomas Parnell</b>, IBM Research – Zurich, Switzerland
</p>
<p><b>Introduction to Snap Machine Learning</b></p>
<p><b>Abstract:</b>  Generalized linear models, such as logistic regression and support vector machines, remain some of the most widely-used techniques in the machine learning field. Their enduring popularity can be attributed to their desirable theoretical properties, effective training algorithms, and relative ease of interpretability. In this talk we will introduce Snap Machine Learning: a new library for fast training of such models, that is designed to enable new real-time and large-scale applications. The library was designed from the ground up with performance in mind. It exploits parallelism at three different levels: across multiple machines in a network, across heterogeneous compute nodes within a machine (e.g. CPU and GPU), as well as the massive parallelism offered by modern GPUs. In this talk we will review this new architecture and give examples of how the library can be used via the various APIs that are provided (e.g. Python, Apache Spark, MPI). Finally, we will present benchmarking results using the publicly available Terabyte Click Logs dataset (from Criteo Labs) and show that Snap Machine Learning can train a logistic regression classifier in 1.53 minutes, 46x faster than any of the results that have been previously reported using the same dataset.</p>

<p><b>Bio:</b>
Thomas received his B.Sc. and Ph.D. degrees in mathematics from the University of Warwick. U.K., in 2006 and 2011, respectively. He joined Arithmatica, Warwick, U.K., in 2005, where he was involved in FPGA design and electronic design automation. In 2007, he co-founded Siglead Europe, a U.K.-limited subsidiary of Yokohama-based Siglead Inc., where he was involved in developing signal processing and error-correction algorithms for HDD, flash, and emerging storage technologies. In 2013, he joined IBM Research in Zürich, Switzerland, where he is actively involved in the research and development of machine learning, compression and error-correction algorithms for IBM’s storage and AI products. His research interests include signal processing, information theory, machine learning and recommender systems.</p>


      <div id="keynote4">
        <h4>Invited talk 4</h4>
      </div>
<p>
<b>Wei Tan</b>, Senior Research Engineer, Citadel LLC
</p>
<p><b>Matrix Factorization on GPUs: A Tale of Two Algorithms</b></p>

<p><b>Abstract:</b> Matrix factorization (MF) is an approach to derive latent features from observations. It is at the heart of many algorithms, e.g., collaborative filtering, word embedding and link prediction. Alternating least Square (ALS) and stochastic gradient descent (SGD) are the two popular methods in solving MF. SGD converges fast, while ALS is easy to parallelize and able to deal with non-sparse ratings. In this talk, I will introduce cuMF(https://github.com/cuMF/), a CUDA-based matrix factorization library that accelerates both ALS and SGD to solve very large-scale MF. cuMF uses a set of techniques to maximize the performance on single and multiple GPUs. These techniques include smart access of sparse data leveraging memory hierarchy, using data parallelism with model parallelism, approximate algorithms and storage. With only a single machine with up to four Nvidia GPU cards, cuMF can be 10 times as fast, and 100 times as cost-efficient, compared with the state-of-art distributed CPU solutions. In this talk I will also share lessons learned in accelerating compute- and memory-intensive kernels on GPUs.</p>

<p><b>Bio:</b> Dr. WEI TAN is Senior Research Engineer at Citadel LLC. Before joining Citadel he was a Research Staff Member at IBM T.J. Watson Research Center. Wei has a wide range of research interests in distributed computing, machine learning, and GPU computing. Specifically, he worked on GPU accelerated platform for large-scale machine learning. He developed cuMF, by far the fastest matrix factorization library on GPUs. His work has been incorporated into IBM patent portfolio and software products such as Spark, BigInsights and Cognos. He received the IEEE Peter Chen Big Data Young Researcher Award (2016), IBM Outstanding Technical Achievement Award (2017, 2016, 2014), Best Paper Award at IEEE SCC (2017, 2011) and ACM/IEEE ccGrid (2015), Best Student Paper Award at IEEE ICWS (2014), Pacesetter Award from Argonne National Laboratory (2010), and caBIG Teamwork Award from the National Institute of Health (2008). He held adjunct professor positions at Tsinghua University and Tianjin University. For more information, please visit <a href="http://wei-tan.github.io/">http://wei-tan.github.io/</a>.</p>

-->
<br>
<div id="update">
<h4><b><mark style="background-color:Yellow;">Update:</mark> The paper submission deadline is extended to October 7th, 2020!</b></h4>
<br>
<div id="introduction">
<h4><b>Introduction</b></h4>
</div>
With the proliferation of artificial intelligence (AI) and machine learning (ML) in every aspect of our automated, digital, and interconnected society, the issues of fairness, explainability and interpretability of AI and ML algorithms have become very important. If the output of an algorithm in response to a query is not transparent to or interpretable by humans, then they will always have questions about it’s correctness and fairness. An algorithm is considered to be fair, if its results are independent of some sensitive but unrelated variables (e.g., gender, race, ethnicity, sexual orientation). For an algorithm to be interpretable, it should not only output the results, but also produce a certificate showing that the results that it has computed are according to the expected specifications. This is our motivation to organize the International Workshop on Fair and Interpretable Learning Algorithms (FILA 2020).

<p><br>The objectives of the FILA 2020 workshop are to:
<ul>
    <li>Provide a venue for academic researchers, industry professionals, and government partners to come together, present and discuss research results, use cases, innovative ideas, challenges, and opportunities that arise from designing machine learning applications and big data analytics solutions using novel, efficient, scalable, fair and interpretable AI and ML algorithms.</li>
    <li>Foster collaboration between Algorithms | Theoretical Computer Science communities and Artificial Intelligence | Machine Learning | Data Science | Network Science communities.</li>
</ul>


<!-- <div id="program">
<h4>Program (August 22, 2019)</h4>
</div>

8am - 8:05am: Introduction to ParLearning 2019
<br><b>8:05am - 9am: Keynote talk 1: Accelerating Deep Learning with Tensor Processing Units - Dr. Lifeng Nai (Google Research, Mountain View, CA, USA)</b>
      <br><p class="tab"><i>Abstract</i>: Google's Tensor Processing Unit (TPU), first deployed in 2015, provides services today for more than one billion people and provides more than an order of magnitude improvement in performance and performance/Watt compared to contemporary platforms. Inspired by the success of the first TPU for neural network inference, Google has developed multiple generations of machine learning supercomputers for neural network training that allow near linear scaling of ML workloads running on TPUv2 and TPUv3. In this talk, we will present how TPU works as a machine learning supercomputer to benefit a growing number of Google services. We will have a deep dive into TPU’s system & chip architecture and our hardware/software codesign methodology that turns accelerator concepts into reality.
          </p>
<br>9am - 9:30am: Regular paper 1: Large Scale Cloud Deployment of Spectral Topic Modeling
<br>9:30am - 10am: Coffee break
<br><b>10am - 10:45am: Keynote talk 2: Bots, Socks and Vandals: Malicious Actors in Online Forums - Professor V.S. Subrahmanian (Dartmouth College, Hanover, NH, USA)</b>
<br><p class="tab"><i>Abstract</i>: Malicious actors are omnipresent in online social and crowdsourced platforms – vandals on Wikipedia, bots on Twitter, and trolls on various platforms all play a major role in degrading the quality of open information and free discussion on the web.This talk will focus on the role of semantics and its relationship with networks in order to classify users on Twitter as bots and users on Wikipedia as vandals. In the context of Twitter bots, this talk will discuss the DARPA Twitter Bot Challenge and subsequent research. In the context of Wikipedia, Professor will also discuss the vandal early warning system (VEWS) and its role in identifying vandals as early as possible. Time permitting, this talk will discuss malicious actors in other online networks such as Slashdot and/or on e-commerce sites such as Flipkart. The talk reflects joint work with many students and colleagues.
      </p>
      <br><b>10:45am - 11:30am: Keynote talk 3: HW-SW Codesign for AI at Facebook - Dr. Satish Nadathur (Facebook Research, Menlo Park, CA, USA)</b>
      <br><p class="tab"><i>Abstract</i>: In this talk, we will describe the principles of HW-SW codesign at Facebook. We will go over key characteristics of deep learning workloads from a HPC perspective – in terms of how limited they are by compute and memory bandwidth. Based on our growing application needs, we need specialized ASIC solutions to keep up with the workload scale. We will describe the overall building blocks of the Facebook-designed hardware released to the Open Compute Project (OCP) that efficiently helps us deploy hardware at scale. We will end by sketching some of the key SW challenges in order to best leverage these systems.
      </p>
<br>11:30am - 12pm: Regular paper 2: Expedite Neural Network Training via Software Techniques
<br>12pm - 12:30pm: Regular paper 3: Scaling up Stochastic Gradient Descent for Non-convex Optimisation
 -->
<br>
<br />
<div id="call">
<h4><b>Call for Papers</b></h4>
</div>

<div>
  <p>
  The <i>International Workshop on Fair and Interpretable Learning Algorithms (FILA 2020)</i> will provide a venue for academic researchers, industry professionals, and government partners to come together, present and discuss research results, use cases, innovative ideas, challenges and opportunities that arise from designing machine learning algorithms that are fair and interpretable. Since the task is inherently multi-disciplinary, the workshop will attempt to foster collaboration between different research communities working on problems ranging from Algorithms, Theoretical Computer Science, Network Science to Artificial Intelligence, Machine Learning, Data Science, and Social Choice, Game Theory, Computational Social Science. This year, our focus is specifically on fairness. We encourage submissions spanning the full range of theoretical and applied works. Topics of interest include, but are not limited to:</p>

  Identification of unfairness and biases
  <ul>
      <li>Biases in popularly used machine learning datasets</li>
      <li>Fairness audits on the use of sensitive data</li>
      <li>Biases in news and social media</li>
      <li>Investigation of black-box systems, particularly web platforms and algorithms</li>
      <li>Biases in machine learning from complex data (e.g., networks, time series)</li>
      <li>Biases in information retrieval and natural language processing</li>
      <li>Other novel application domains such as economics, healthcare, climate studies</li>
      <li>Machine learning in the context of developing countries and other under-represented communities</li>
  </ul>

  Designing fair learning algorithms
  <ul>
      <li>The statistical and computational complexity of fair machine learning</li>
      <li>Online and stochastic optimization methods for fair machine learning</li>
      <li>Fair machine learning through Bayesian methods</li>
      <li>Achieving fairness through computational social choice and game theory</li>
      <li>Fairness, accountability, transparency and ethics in search</li>
      <li>Fairness and diversity in recommender systems</li>
      <li>Fairness-aware algorithms for social impact</li>
      <li>Evaluation methods for fair and interpretable machine learning</li>
      <li>Fairness beyond supervised learning, e.g. clustering, reinforcement learning</li>
      <li>Interpretability of neural network and deep learning algorithms</li>
      <li>Accountability in human-in-the-loop machine learning</li>
  </ul>
</div>
<!-- <p>
Proceedings of the Parlearning workshop will be distributed at the conference and will be submitted for inclusion in the IEEE Xplore Digital Library after the conference.
</p> -->
<!-- 
<a href="PARLEARNING2018.pdf">PDF Flyer</a>
-->

<!-- -------------------------------------------------------------------------------------------------------- 
      <div id="awards">
        <h4>Journal publication</h4>
        
Selected papers from the workshop will be published in a Special Issue of <a href="http://www.journals.elsevier.com/future-generation-computer-systems/">Future Generation Computer Systems</a>, Elsevier's International Journal of eScience. Special Issue papers will undergo additional review.
      </div>
-->


<!-- -------------------------------------------------------------------------------------------------------- -->
<!-- <div id="awards">
  <h4>Awards</h4>
</div>
<p><b>Best Paper Award:</b> The program committee will nominate a paper for the Best Paper award. In <a href="http://parlearning.ecs.fullerton.edu/parlearning2018.html">past years</a>, the Best Paper award included a cash prize. Stay tuned for this year!
<p><b>Travel Awards:</b> Students with accepted papers will get a chance to apply for a travel award. Please find details on the <a href="https://www.kdd.org/kdd2019/">ACM KDD 2019</a> website.      
 -->
<!-- -------------------------------------------------------------------------------------------------------- -->
<br />
      <div id="dates">
        <h4><b>Important Dates</b></h4>
      </div>
      <ul>
        <li>Paper Submission: October 7, 2020</li>
        <li>Notification: November 1, 2020 </li>
        <li>Camera Ready: November 15, 2020 </li>
        <li>Workshop: December 10 - 13, 2020 </li>
      </ul>
      All times are 11:59 PM Eastern Standard Time.

<!-- -------------------------------------------------------------------------------------------------------- -->
      <br>
      <br />
      <div id="guidelines">

        <h4><b>Paper Guidelines</b></h4>
      </div>
      <div>
        <ul>
          <li>
            <div>
              <h5><b>Paper Submission Guidelines</b></h5>
              <p>
                We welcome submissions related to the above research areas. Submissions may include late-breaking results and work in progress. We also solicit vision or position papers. As IEEE BigData 2020 will no longer take place physically in Atlanta, Georgia, US, and will instead take place virtually, the relevant workshop submissions will be accepted for <b>video presentations</b> in the virtual presentation session.
              </p>
            </div>
          </li>
          <li>
            <div>
              <h5><b>Submission Site: </b><a href="https://wi-lab.com/cyberchair/2020/bigdata20/scripts/submit.php?subarea=S31&undisplay_detail=1&wh=/cyberchair/2020/bigdata20/scripts/ws_submit.php" target="_blank">Cyberchair's FILA submission portal</a></h5>
              <p>
              Kindly note that we will distinguish short vs full paper based on their length. For non-archival paper submissions, kindly use title note to indicate the type of submission. We will also double check with the authors of the accepted papers at the time of camera ready submission.
          	  </p>
            </div>
          </li>
          <li>
            <div>
              <h5><b>Archival and Non-archival</b></h5>
              <p>
                FILA 2020 offers authors the choice of archival and non-archival paper submissions. Archival papers will appear in the published proceedings of the workshop, if they are accepted. Whereas, accepted non-archival papers will only appear in the workshop website and not in the proceedings. <b>Authors of non-archival papers are free to also submit their work for publication elsewhere.</b> Note that all submissions will be judged by the same quality standards, regardless of whether the authors choose the archival or non-archival option. Authors of all accepted papers must register and present their work at the workshop, regardless of whether their paper is archival or non-archival. For non-archival paper submissions, kindly use title note to indicate the type of submission.
              </p>
            </div>
          </li>
          <li>
            <div>
              <h5><b>Long and Short Papers</b></h5>
              <p>
                Paper authors may choose between two formats: Long (10 pages) and Short (4 pages), in the IEEE Computer Society proceedings manuscript format. Both formats will be rigorously peer-reviewed. We will distinguish long vs. short paper based on their length.
              </p>
            </div>
          </li>
          <li>
            <div>
              <h5><b>Writing Guidelines</b></h5>
              <p>
                All submissions must follow the IEEE Computer Society proceedings manuscript format. You are strongly encouraged to print and double-check your PDF file before its submission, especially if your paper contains Asian/European language symbols (such as Chinese/Korean characters or English letters with European fonts).  Complete papers are required; abstracts and incomplete papers will not be reviewed. FILA follows a single-blind review process. The formatting guidelines are available at: <a href="https://www.ieee.org/conferences/publishing/templates.html" target="_blank">https://www.ieee.org/conferences/publishing/templates.html</a> 
              </p>
            </div>
          </li>
        </ul>
      </div>


<!-- -------------------------------------------------------------------------------------------------------- -->
      <br />
      <div id="keynote">
        <h4><b>Keynote Speakers</b></h4>
      </div>
      <ul>
        <li><a href="https://people.mpi-sws.org/~gummadi/" target="_blank">Professor Krishna P. Gummadi</a> (Max Planck Institute for Software Systems, Saarbrücken, Germany)</li>
        <li><a href="http://tonghanghang.org/" target="_blank">Professor Hanghang Tong</a> (University of Illinois at Urbana-Champaign, Urbana, IL, USA)</li>
        <li><a href="https://coe.northeastern.edu/people/ganguly-auroop/" target="_blank">Professor Auroop Ratan Ganguly</a> (Northeastern University, Boston, MA, USA)</li>
      </ul>

      <br />
      <div id="organization">
        <h4><b>Organization</b></h4>
      </div>
      <p>
      <ul>
      <li><b>General Chairs:</b> <a href="http://www.cse.iitd.ac.in/~arindamp/" target="_blank">Arindam Pal</a> (Data61, CSIRO and Cyber Security CRC, Sydney, New South Wales, Australia) and <a href="https://sites.google.com/site/yinglongxia/" target="_blank">Yinglong Xia</a> (Facebook AI, Santa Clara, CA, USA)</li>
      <li><b>Program Chairs:</b> <a href="https://people.mpi-sws.org/~achakrab/" target="_blank">Abhijnan Chakraborty</a> (Max Planck Institute for Software Systems, Saarbrücken, Germany) and <a href="http://mayank4490.github.io/" target="_blank">Mayank Singh</a> (IIT Gandhinagar, Gujarat, India)</li>
      <li><b>Web and Publicity Chairs:</b> <a href="https://shruti-singh.github.io/" target="_blank">Shruti Singh</a> (IIT Gandhinagar, Gujarat, India)</li>
      <li><b>Technical Program Committee</b>:
      	<ul>
      		<li>Abhisek Dash (IIT Kharagpur, India)
      		<li>Ana-Andreea	Stoica (Columbia University, USA)
      		<li>Ancsa Hannák (University of Zurich, Switzerland)
      		<li>Animesh	Mukherjee (IIT Kharagpur, India)
      		<li>Arpita	Biswas (Google Research, India)
      		<li>Chayan	Sarkar (TCS Research and Innovation)
      		<li>Dongxi	Liu (Data61, CSIRO)
      		<li>Gourab	Patro (IIT Kharagpur, India)
      		<li>Jiaojiao Jiang	(University of New South Wales)
      		<li>Kripa Ghosh (IISER Kolkata, India)
      		<li>Maunendra Desarkar (IIT Hyderabad, India)
      		<li>Nina Grgic-Hlaca (MPI SWS, Germany)
      		<li>Nipun Batra (IIT Gandhinagar, India)
      		<li>Saptarshi Ghosh (IIT Kharagpur, India)
      		<li>Sharif	Abuadbba (Data61, CSIRO)
      		<li>Suranga	Seneviratne (University of Sydney)
      		<li>Sushmita Ruj (Data61, CSIRO)
      		<li>Tanmoy	Chakraborty	(IIIT Delhi)
      		<li>Till Speicher (MPI SWS, Germany)
      		<li>Udit Bhatia	(IIT Gandhinagar, India)
      	</ul>
      </ul>
      </p>

<!--      
      <div>
        <h4>Technical Program Committee</h4>
      </div>
      <p>
      <ul>
      <li>Vito Giovanni Castellana, Pacific Northwest National Laboratory, USA
      <li>Tanmoy Chakraborty, IIIT Delhi, India
      <li>Sutanay Choudhury, Pacific Northwest National Laboratory, USA
      <li>Erich Elsen, Google Brain, USA
      <li>Dinesh Garg, IIT Gandhinagar and IBM Research, India
      <li>Kripabandhu Ghosh, IIT Kanpur, India
      <li>Saptarshi Ghosh, IIT Kharagpur, India
      <li>Kazuaki Ishizaki, IBM Research - Tokyo, Japan
      <li>Debnath Mukherjee, TCS Research, India
      <li>Francesco Parisi, University of Calabria, Italy
      <li>Saurabh Paul, PayPal, USA
      <li>Jianting Zhang, City College of New York, USA
      </ul>
      </p>

-->
<br />
<p>



    </div><!-- /.container -->





  </div>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <!-- <script src="../../dist/js/bootstrap.min.js"></script> -->
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="http://getbootstrap.com/assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
